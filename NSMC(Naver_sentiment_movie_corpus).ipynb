{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShZshPOXYrlp",
        "outputId": "ee8ef7fc-eea7-4c10-cfcf-b33d23690300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'nsmc_study' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/simonjisu/nsmc_study.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torchtext==0.10.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGM3POVDbRum",
        "outputId": "75947117-934b-4c2c-89a3-36795d6aa022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext==0.10.0 in /usr/local/lib/python3.9/dist-packages (0.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchtext==0.10.0) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchtext==0.10.0) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torchtext==0.10.0) (4.65.0)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.9/dist-packages (from torchtext==0.10.0) (1.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.9.0->torchtext==0.10.0) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.10.0) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.10.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.10.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.10.0) (1.26.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBPA564ubR6c",
        "outputId": "9a26fa2e-a800-4a56-d4e0-5a4d48cfaef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.9/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from konlpy) (1.4.1)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.9/dist-packages (from konlpy) (4.9.2)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.9/dist-packages (from konlpy) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from torchtext.legacy import data\n",
        "from sklearn.model_selection import train_test_split    \n",
        "from copy import deepcopy\n",
        "\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQzcpaORbRyj",
        "outputId": "5a160587-d378-4254-e906-194f25135694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "        \"batch_size\": 64,\n",
        "        \"gpu_id\": 0,\n",
        "        \"rnn\": True,\n",
        "        \"hidden_size\": 256,\n",
        "        \"n_layers\": 2,\n",
        "        \"cnn\": True,\n",
        "        \"use_batch_norm\": True,\n",
        "        \"window_size\":[3, 4, 5],\n",
        "        \"n_filters\":[100, 100, 100],\n",
        "        \"word_vector_size\": 300,\n",
        "        \"dropout\":0.3,\n",
        "        \"n_epochs\":5,\n",
        "        \"verbose\": 1,\n",
        "        \"model_fn\": \"./model.pth\",\n",
        "        \"max_length\":256\n",
        "}"
      ],
      "metadata": {
        "id": "GjPyT0Nf2avp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "tokenizer = Okt()"
      ],
      "metadata": {
        "id": "pYm-Cni3bR92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# utils"
      ],
      "metadata": {
        "id": "yNUjR3MhwuDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_func(sentence):\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    sentence = sentence.strip()\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "xdbu0bpUxLdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_dataset(input_data, text, label):\n",
        "    list_of_example = [data.Example.fromlist(row.tolist(), fields=[('text', text), ('label', label)])  for _, row in input_data.iterrows()]\n",
        "    dataset = data.Dataset(examples=list_of_example, fields=[('text', text), ('label', label)])\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "1ICK8Zsexvjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT = data.Field(sequential=True, use_vocab=True, tokenize=tokenizer.morphs, lower=False, batch_first=True, fix_length=50)\n",
        "LABEL = data.LabelField(dtype = torch.float)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train = pd.read_csv(\"/content/nsmc_study/data/ratings_train.txt\", sep='\\t')\n",
        "test = pd.read_csv(\"/content/nsmc_study/data/ratings_test.txt\", sep='\\t')\n",
        "\n",
        "train = train.drop(columns=['id'])\n",
        "test = test.drop(columns=['id'])\n",
        "\n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "\n",
        "train_data = train.dropna() #말뭉치에서 nan 값을 제거함\n",
        "test_data  = test.dropna()\n",
        "\n",
        "\n",
        "train_data['document'] = train_data['document'].apply(preprocess_func)\n",
        "test_data['document']  = test_data['document'].apply(preprocess_func)\n",
        "\n",
        "# split data\n",
        "train_data, valid_data = train_test_split(train_data, test_size=0.3, random_state=32)\n",
        "print(len(train_data))\n",
        "print(len(valid_data))\n",
        "print(len(test_data))\n",
        "\n",
        "print(train_data.shape)\n",
        "print(valid_data.shape)  \n",
        "print(test_data.shape)\n",
        "\n",
        "train_data = convert_dataset(train_data, TEXT, LABEL)\n",
        "valid_data = convert_dataset(valid_data, TEXT, LABEL)\n",
        "test_data  = convert_dataset(test_data, TEXT, LABEL)\n",
        "\n",
        "\n",
        "print(f'Number of training examples   : {len(train_data)}')\n",
        "print(f'Number of validation examples : {len(valid_data)}')\n",
        "print(f'Number of testing examples    : {len(test_data)}')\n",
        "\n",
        "\n",
        "MAX_VOCAB_SIZE = 20000\n",
        "\n",
        "# 단어 사전 생성\n",
        "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "print(f\"Unique tokens in TEXT vocabulary : {len(TEXT.vocab)}\")\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
        "\n",
        "train_loader, valid_loader, test_loader = data.Iterator.splits(\n",
        "  (train_data, valid_data, test_data),\n",
        "  batch_size = BATCH_SIZE,\n",
        "  sort = False,\n",
        "  device = device)\n",
        "\n",
        "print('Number of minibatch for training dataset   : {}'.format(len(train_loader)))\n",
        "print('Number of minibatch for validation dataset : {}'.format(len(valid_loader)))\n",
        "print('Number of minibatch for testing dataset    : {}'.format(len(test_loader)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ufcd39M6wvz9",
        "outputId": "da2a039c-48ed-47d2-b552-f5a4b0ebfdd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(150000, 2)\n",
            "(50000, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-77-85b4b7cb7ef3>:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_data['document'] = train_data['document'].apply(preprocess_func)\n",
            "<ipython-input-77-85b4b7cb7ef3>:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_data['document']  = test_data['document'].apply(preprocess_func)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "104996\n",
            "44999\n",
            "49997\n",
            "(104996, 2)\n",
            "(44999, 2)\n",
            "(49997, 2)\n",
            "Number of training examples   : 104996\n",
            "Number of validation examples : 44999\n",
            "Number of testing examples    : 49997\n",
            "Unique tokens in TEXT vocabulary : 20002\n",
            "Unique tokens in LABEL vocabulary: 2\n",
            "Number of minibatch for training dataset   : 1641\n",
            "Number of minibatch for validation dataset : 704\n",
            "Number of minibatch for testing dataset    : 782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#model"
      ],
      "metadata": {
        "id": "uAzZ585-0KWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(nn.Module):\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        word_vector_size,\n",
        "        hidden_size,\n",
        "        n_classes,\n",
        "        n_layers=4,\n",
        "        dropout_p=0.4\n",
        "    ):\n",
        "        self.input_size = input_size\n",
        "        self.word_vector_size = word_vector_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_classes = n_classes\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout_p = dropout_p\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.emb = nn.Embedding(input_size, word_vector_size)\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=self.word_vector_size,\n",
        "            hidden_size=self.hidden_size,\n",
        "            num_layers=self.n_layers,\n",
        "            dropout=self.dropout_p,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        \n",
        "        self.linear_layer = nn.Linear(self.hidden_size*2, self.n_classes)\n",
        "        self.softmax_layer = nn.LogSoftmax(dim=-1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # |x| = (batch_size, length)\n",
        "        x = self.emb(x)\n",
        "        # |x| = (batch_size, length, word_vector_size)\n",
        "        x, _ = self.rnn(x)\n",
        "        # |x| = (batch_size, length, hidden_size*2)\n",
        "        x = self.linear_layer(x[:,-1])\n",
        "        # |x| = (batch_size, n_classes)\n",
        "        y = self.softmax_layer(x)\n",
        "        # |x| = (batch_size, n_classes)\n",
        "        \n",
        "        return y\n",
        "        "
      ],
      "metadata": {
        "id": "N0K2sM66bknI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#trainer"
      ],
      "metadata": {
        "id": "A3wpLQEg0MRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer():\n",
        "\n",
        "  def __init__(self, model, optimizer, crit):\n",
        "      self.model = model\n",
        "      self.optimizer = optimizer\n",
        "      self.crit = crit\n",
        "\n",
        "      super().__init__()\n",
        "\n",
        "  def _train(self, train_loader, config):\n",
        "      self.model.train()\n",
        "\n",
        "      total_loss = 0\n",
        "\n",
        "      for i, (x_i, y_i) in enumerate(train_loader):\n",
        "          y_hat_i = self.model(x_i)\n",
        "          loss_i = self.crit(y_hat_i, y_i.squeeze().long())\n",
        "\n",
        "          # Initialize the gradients of the model.\n",
        "          self.optimizer.zero_grad()\n",
        "          loss_i.backward()\n",
        "\n",
        "          self.optimizer.step()\n",
        "          \n",
        "          if config['verbose'] >= 2:\n",
        "              print(\"Train Iteration(%d/%d): loss=%.4e\" % (i + 1, len(train_loader), float(loss_i)))\n",
        "\n",
        "          # Don't forget to detach to prevent memory leak.\n",
        "          total_loss += float(loss_i)\n",
        "\n",
        "      return total_loss / len(train_loader)\n",
        "\n",
        "  def _validate(self, valid_loader, config):\n",
        "      # Turn evaluation mode on.\n",
        "      self.model.eval()\n",
        "\n",
        "      # Turn on the no_grad mode to make more efficintly.\n",
        "      with torch.no_grad():\n",
        "          total_loss = 0\n",
        "\n",
        "          for i, (x_i, y_i) in enumerate(valid_loader):\n",
        "              y_hat_i = self.model(x_i)\n",
        "              loss_i = self.crit(y_hat_i, y_i.squeeze().long())\n",
        "              \n",
        "              if config['verbose'] >= 2:\n",
        "                  print(\"Valid Iteration(%d/%d): loss=%.4e\" % (i + 1, len(valid_loader), float(loss_i)))\n",
        "\n",
        "              total_loss += float(loss_i)\n",
        "\n",
        "          return total_loss / len(valid_loader)\n",
        "\n",
        "  def train(self, train_loader, valid_loader, config):\n",
        "      lowest_loss = np.inf\n",
        "      best_model = None\n",
        "\n",
        "      for epoch_index in range(config['n_epochs']):\n",
        "          train_loss = self._train(train_loader, config)\n",
        "          valid_loss = self._validate(valid_loader, config)\n",
        "\n",
        "          # You must use deep copy to take a snapshot of current best weights.\n",
        "          if valid_loss <= lowest_loss:\n",
        "              lowest_loss = valid_loss\n",
        "              best_model = deepcopy(self.model.state_dict())\n",
        "\n",
        "          print(\"Epoch(%d/%d): train_loss=%.4e  valid_loss=%.4e  lowest_loss=%.4e\" % (\n",
        "              epoch_index + 1,\n",
        "              config['n_epochs'],\n",
        "              train_loss,\n",
        "              valid_loss,\n",
        "              lowest_loss,\n",
        "          ))\n",
        "\n",
        "      # Restore to best model.\n",
        "      self.model.load_state_dict(best_model)\n",
        "      return self.model\n"
      ],
      "metadata": {
        "id": "YuvJswnTbkrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(config):\n",
        "    print(TEXT.vocab.freqs.most_common(20))\n",
        "    print(TEXT.vocab.itos[:10])\n",
        "    print(LABEL.vocab.stoi)\n",
        "    \n",
        "    if config[\"rnn\"] is False and config[\"cnn\"] is False:\n",
        "        raise Exception(\"you should select the rnn model or cnn model or both\")\n",
        "\n",
        "    if config[\"rnn\"]:\n",
        "        model = RNNModel(\n",
        "            input_size=len(TEXT.vocab),\n",
        "            word_vector_size=256,\n",
        "            hidden_size=128,\n",
        "            n_classes=len(LABEL.vocab),\n",
        "            n_layers=2,\n",
        "            dropout_p=0.3\n",
        "\n",
        "        )\n",
        "        optimizer = optim.Adam(model.parameters())\n",
        "        crit = nn.NLLLoss()\n",
        "        print(model)\n",
        "        \n",
        "        if config[\"gpu_id\"] >= 0:\n",
        "            model.cuda(config[\"gpu_id\"])\n",
        "            crit.cuda(config[\"gpu_id\"])\n",
        "        rnn_trainer = Trainer(model, optimizer, crit)\n",
        "\n",
        "        rnn_model = rnn_trainer.train(\n",
        "            train_loader,\n",
        "            valid_loader,\n",
        "            config\n",
        "        )\n",
        "\n",
        "    torch.save(\n",
        "        {\n",
        "            'rnn': rnn_model.state_dict() if config[\"rnn\"] else None,\n",
        "            'config': config,\n",
        "            'vocab': TEXT.vocab,\n",
        "            'classes': LABEL.vocab\n",
        "        }, config[\"model_fn\"]\n",
        "    )"
      ],
      "metadata": {
        "id": "61WsjBiwbkuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(config)"
      ],
      "metadata": {
        "id": "LjpQW-mUbkw5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1afc037b-5c8d-4119-8475-b605ba2d0131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('.', 169097), ('이', 39289), ('영화', 35555), ('!', 22218), ('의', 21676), ('가', 19298), ('에', 18786), ('을', 16184), (',', 15841), ('도', 14951), ('들', 13303), ('?', 12819), ('는', 12332), ('를', 11339), ('은', 11165), ('너무', 7775), ('한', 7643), ('다', 7209), ('정말', 6885), ('적', 6135)]\n",
            "['<unk>', '<pad>', '.', '이', '영화', '!', '의', '가', '에', '을']\n",
            "defaultdict(None, {0: 0, 1: 1})\n",
            "RNNModel(\n",
            "  (emb): Embedding(20002, 256)\n",
            "  (rnn): LSTM(256, 128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
            "  (linear_layer): Linear(in_features=256, out_features=2, bias=True)\n",
            "  (softmax_layer): LogSoftmax(dim=-1)\n",
            ")\n",
            "Epoch(1/5): train_loss=5.4586e-01  valid_loss=4.0652e-01  lowest_loss=4.0652e-01\n",
            "Epoch(2/5): train_loss=3.4921e-01  valid_loss=3.5634e-01  lowest_loss=3.5634e-01\n",
            "Epoch(3/5): train_loss=2.7910e-01  valid_loss=3.5643e-01  lowest_loss=3.5634e-01\n",
            "Epoch(4/5): train_loss=2.2752e-01  valid_loss=3.6463e-01  lowest_loss=3.5634e-01\n",
            "Epoch(5/5): train_loss=1.8194e-01  valid_loss=4.2292e-01  lowest_loss=3.5634e-01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# test"
      ],
      "metadata": {
        "id": "pEtLD3tV3SV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_fn = \"./model.pth\"\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "2dCZAflrbk3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load(fn, device):\n",
        "  d = torch.load(fn, map_location=device)\n",
        "  \n",
        "  return d['rnn'], d['config'], d['vocab'], d['classes']"
      ],
      "metadata": {
        "id": "UGN8owCEbk5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader):\n",
        "  model.eval()\n",
        "  \n",
        "  correct_cnt, total_cnt = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      y_hat = model(x)\n",
        "      correct_cnt += (y.squeeze() == torch.argmax(y_hat, dim=-1)).sum()\n",
        "      total_cnt += len(x)\n",
        "\n",
        "    accuracy = correct_cnt / total_cnt\n",
        "    print(\"Accuracy: %.4f\" % accuracy)\n",
        "    "
      ],
      "metadata": {
        "id": "gmQ9t1Yhbk8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dict, train_config, vocab, classes = load(model_fn, device)\n",
        "\n",
        "model = RNNModel(\n",
        "                input_size=len(vocab),\n",
        "                word_vector_size=256,\n",
        "                hidden_size=128,\n",
        "                n_classes=len(classes),\n",
        "                n_layers=2,\n",
        "                dropout_p=0.3\n",
        "        )\n",
        "model.load_state_dict(model_dict)\n",
        "print(model)\n",
        "\n",
        "test(model.to(device), test_loader)"
      ],
      "metadata": {
        "id": "7KSpG2tIbk_Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15c8455a-b57a-46b3-f0d2-81f5b2e61936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNNModel(\n",
            "  (emb): Embedding(20002, 256)\n",
            "  (rnn): LSTM(256, 128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
            "  (linear_layer): Linear(in_features=256, out_features=2, bias=True)\n",
            "  (softmax_layer): LogSoftmax(dim=-1)\n",
            ")\n",
            "Accuracy: 0.8389\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model,sentence):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        sent = tokenizer.morphs(sentence)\n",
        "        sent = torch.tensor([TEXT.vocab.stoi[i] for i in sent])\n",
        "        sent = F.pad(sent,pad = (1,50-len(sent)-1),value = 1)\n",
        "        sent = sent.unsqueeze(dim = 0) #for batch\n",
        "        output = torch.argmax(model(sent), dim=-1)\n",
        "        print(torch.exp(model(sent)))\n",
        "        \n",
        "        return output.item()\n",
        "        \n",
        "examples = [\n",
        "  \"딥러닝 수업 오늘 하기 좋네요!\",\n",
        "  \"독감 정말 싫어요\",\n",
        "  \"영화가 정말 재밌네요\",\n",
        "  \"영화가 재밌습니다\",\n",
        "  \"최악의 영화\"\n",
        "]\n",
        "\n",
        "model = model.to('cpu')\n",
        "for idx in range(len(examples)) :\n",
        "\n",
        "    sentence = examples[idx]\n",
        "    pred = predict(model,sentence)\n",
        "    print(\"\\n\",sentence)\n",
        "    if pred == 1 :\n",
        "        print(f\">>>긍정 리뷰입니다. ({pred : .2f})\")\n",
        "    else:\n",
        "        print(f\">>>부정 리뷰입니다.({pred : .2f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTZ16cs9ASNX",
        "outputId": "5ddd149b-4390-428f-db41-1c643c5cb819"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0223, 0.9777]])\n",
            "\n",
            " 딥러닝 수업 오늘 하기 좋네요!\n",
            ">>>긍정 리뷰입니다. ( 1.00)\n",
            "tensor([[0.7653, 0.2347]])\n",
            "\n",
            " 독감 정말 싫어요\n",
            ">>>부정 리뷰입니다.( 0.00)\n",
            "tensor([[0.0498, 0.9502]])\n",
            "\n",
            " 영화가 정말 재밌네요\n",
            ">>>긍정 리뷰입니다. ( 1.00)\n",
            "tensor([[0.1119, 0.8881]])\n",
            "\n",
            " 영화가 재밌습니다\n",
            ">>>긍정 리뷰입니다. ( 1.00)\n",
            "tensor([[0.9819, 0.0181]])\n",
            "\n",
            " 최악의 영화\n",
            ">>>부정 리뷰입니다.( 0.00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TmKKQXVSWDca"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}